{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas matplotlib statsmodels scikit-learn joblib numba openpyxl yfinance\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import yfinance as yf  # <-- yfinance for data\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "CSV_FILE_PATH = '/workspaces/Stock-Market-Prediction/Data/S&P500 Stocks.csv'\n",
    "\n",
    "today_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "OUTPUT_FILE_PATH = f'/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/analysis_results_{today_date}.xlsx'\n",
    "PLOT_DIR = 'QI Output/plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Set END_DATE to tomorrow to include today's data, since yfinance's end date is exclusive\n",
    "END_DATE = (datetime.datetime.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "START_DATE = '1980-01-01'  # Adjust as needed\n",
    "\n",
    "def load_tickers(csv_path):\n",
    "    \"\"\"\n",
    "    Loads tickers and descriptions from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: List of ticker symbols.\n",
    "    - Dict[str, str]: Dictionary mapping ticker symbols to their descriptions.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"The CSV file '{csv_path}' does not exist. Please check the path.\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    required_columns = {'Symbol', 'Description'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV file must contain columns: {required_columns}\")\n",
    "\n",
    "    # Handle duplicates\n",
    "    df = df.drop_duplicates(subset='Symbol')\n",
    "\n",
    "    tickers = df['Symbol'].tolist()\n",
    "    descriptions = dict(zip(df['Symbol'], df['Description']))\n",
    "\n",
    "    print(f\"Loaded {len(tickers)} tickers from '{csv_path}'.\")\n",
    "    return tickers, descriptions\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Fetch Data via yfinance --> Could use FMP API or Bloomberg but yfinance seems to be quickest as there are no direct API calls\n",
    "# --------------------------------------------------------------------------\n",
    "def fetch_data_with_yfinance(tickers, start_date='1980-01-01', end_date=None):\n",
    "    \"\"\"\n",
    "    Fetches historical Adjusted Close (or 'Close' if auto_adjust=True) prices\n",
    "    for all tickers using yfinance.\n",
    "\n",
    "    Parameters:\n",
    "    - tickers (List[str]): List of ticker symbols.\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format (default: tomorrow's date).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame (indexed by date) with columns = ticker symbols.\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = (datetime.datetime.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"Fetching data from yfinance for {len(tickers)} ticker(s)...\")\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        progress=False,\n",
    "        group_by='ticker',\n",
    "        auto_adjust=True\n",
    "    )\n",
    "\n",
    "    # Note to self: If only one ticker, data won't be multi-index\n",
    "    if len(tickers) == 1:\n",
    "        adj_close_df = data['Close'].to_frame(name=tickers[0])\n",
    "    else:\n",
    "        # Note to self: If multiple tickers, data is multi-index\n",
    "        adj_close_df = pd.DataFrame()\n",
    "        for tkr in tickers:\n",
    "            if ('Close' in data[tkr]):\n",
    "                adj_close_df[tkr] = data[tkr]['Close']\n",
    "\n",
    "    # IMPORTANT Drop rows where all tickers are NaN so that we don't have empty rows otherwise will be issues with rolling calculations\n",
    "    adj_close_df.dropna(how='all', inplace=True)\n",
    "\n",
    "    return adj_close_df\n",
    "\n",
    "try:\n",
    "    TICKERS, TICKER_DESCRIPTIONS = load_tickers(CSV_FILE_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tickers: {e}\")\n",
    "    TICKERS = []\n",
    "    TICKER_DESCRIPTIONS = {}\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "def preprocess_ticker_data(series):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for a single ticker by forward-filling,\n",
    "    ensuring a business-day frequency, and dropping NaN.\n",
    "    \"\"\"\n",
    "    series = series.dropna()\n",
    "    series = series.asfreq('B')\n",
    "    series.fillna(method='ffill', inplace=True)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(series.index):\n",
    "        series.index = pd.to_datetime(series.index)\n",
    "    return series\n",
    "\n",
    "\n",
    "def log_transform(series):\n",
    "    \"\"\"Apply natural log transform, avoiding log(0) errors.\"\"\"\n",
    "    series = series.replace(0, 1e-9)\n",
    "    return np.log(series)\n",
    "\n",
    "# Decomposition\n",
    "\n",
    "def decompose_time_series(series, model='additive', period=252):\n",
    "    \"\"\"\n",
    "    Decomposes the time series into trend, seasonal, and residual components.\n",
    "    \"\"\"\n",
    "    print(f\"Performing '{model}' decomposition (period={period})...\")\n",
    "    return seasonal_decompose(series, model=model, period=period, extrapolate_trend='freq')\n",
    "\n",
    "\n",
    "def plot_decomposition(result, title='Time Series Decomposition', ticker='', save_dir=PLOT_DIR):\n",
    "    \"\"\"Plot the decomposition results.\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.suptitle(f\"{title} - {ticker}\", fontsize=16)\n",
    "\n",
    "    plt.subplot(411)\n",
    "    plt.plot(result.observed, color='blue', label='Observed')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(412)\n",
    "    plt.plot(result.trend, color='orange', label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(413)\n",
    "    plt.plot(result.seasonal, color='green', label='Seasonal')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(414)\n",
    "    plt.plot(result.resid, color='red', label='Residual')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(os.path.join(save_dir, f\"{ticker}_decomposition.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Trend Slope Calculation (Vectorized Rolling Regression)\n",
    "\n",
    "def calculate_trend_slope_vectorized(trend_series, window=252):\n",
    "    \"\"\"\n",
    "    Calculates rolling linear regression slope on the trend component\n",
    "    using a vectorized approach.\n",
    "    \"\"\"\n",
    "    y = trend_series.dropna().values\n",
    "    n = len(y)\n",
    "    if n < window:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    try:\n",
    "        from numpy.lib.stride_tricks import sliding_window_view\n",
    "        rolling_windows = sliding_window_view(y, window_shape=window)\n",
    "    except AttributeError:\n",
    "        # Due to problems with Numpy --> fallback for older numpy versions\n",
    "        shape = (n - window + 1, window)\n",
    "        strides = (y.strides[0], y.strides[0])\n",
    "        rolling_windows = np.lib.stride_tricks.as_strided(y, shape=shape, strides=strides)\n",
    "\n",
    "    X = np.arange(window)\n",
    "    X_mean = X.mean()\n",
    "    X_var = ((X - X_mean) ** 2).sum()\n",
    "\n",
    "    Y_mean = rolling_windows.mean(axis=1)\n",
    "    cov = (rolling_windows - Y_mean[:, np.newaxis]) @ (X - X_mean)\n",
    "\n",
    "    slope = cov / X_var\n",
    "    idx = trend_series.dropna().index[window-1:]\n",
    "    return pd.Series(data=slope, index=idx)\n",
    "\n",
    "# Residual Z-Score\n",
    "\n",
    "def calculate_residual_zscore(residual_series, window=252):\n",
    "    \"\"\"\n",
    "    Calculates rolling Z-score of the residual component.\n",
    "    \"\"\"\n",
    "    rolling_mean = residual_series.rolling(window=window, min_periods=window).mean()\n",
    "    rolling_std = residual_series.rolling(window=window, min_periods=window).std()\n",
    "    z_score = (residual_series - rolling_mean) / rolling_std\n",
    "    return z_score\n",
    "\n",
    "\n",
    "# Trend Strength Logic to get some context of momentum\n",
    "\n",
    "def get_trend_strength(slope):\n",
    "    \"\"\"\n",
    "    Determines a multiplier based on absolute slope.\n",
    "    \"\"\"\n",
    "    abs_slope = abs(slope)\n",
    "    if abs_slope < 0.01:\n",
    "        return 1.0  # Weak trend\n",
    "    elif 0.01 <= abs_slope < 0.05:\n",
    "        return 1.5  # Moderate trend\n",
    "    else:\n",
    "        return 2.0  # Strong trend\n",
    "\n",
    "# Backtest Strategy with Variable Position Sizing i.e. Z-score of -2 buy +2 sell is default but want to continue adding on further weakness\n",
    "\n",
    "def backtest_strategy(analysis_df, ticker='', description=''):\n",
    "    \"\"\"\n",
    "    Enhanced backtest:\n",
    "    - Buys incrementally when Z-score < -2\n",
    "    - Sells incrementally when Z-score > 2\n",
    "    - Position sizing is influenced by Z-score magnitude * trend strength.\n",
    "    - max_position = 3.0 (example) to limit total position.\n",
    "\n",
    "    Returns:\n",
    "    - performance_metrics: dict\n",
    "    - last_trade_signal_details: (signal_type, signal_date, signal_strength)\n",
    "    \"\"\"\n",
    "    analysis_df['Position'] = 0.0\n",
    "    last_trade_signal_type = 'N/A'\n",
    "    last_signal_date = 'N/A'\n",
    "    last_signal_strength = 'N/A'\n",
    "\n",
    "    position = 0.0\n",
    "    max_position = 3.0  # maximum position size\n",
    "    for idx, row in analysis_df.iterrows():\n",
    "        current_slope = row['Trend_Slope'] if not pd.isna(row['Trend_Slope']) else 0.0\n",
    "        trend_strength = get_trend_strength(current_slope)\n",
    "\n",
    "        z_score = row['Residual_Z_Score']\n",
    "        signal_strength = 0.0\n",
    "        signal_type = None\n",
    "\n",
    "        if z_score < -2:\n",
    "            # Potential Buy\n",
    "            signal_strength = abs(z_score) * trend_strength\n",
    "            # Cap at max position\n",
    "            if position + signal_strength > max_position:\n",
    "                signal_strength = max_position - position\n",
    "\n",
    "            if signal_strength > 0:\n",
    "                signal_type = 'Buy'\n",
    "                position += signal_strength\n",
    "                last_trade_signal_type = signal_type\n",
    "                last_signal_date = idx.strftime('%Y-%m-%d')\n",
    "                last_signal_strength = round(signal_strength, 2)\n",
    "                print(f\"[{ticker}] Buy on {last_signal_date} (Strength: {last_signal_strength})\")\n",
    "\n",
    "        elif z_score > 2:\n",
    "            # Potential Sell\n",
    "            signal_strength = z_score * trend_strength\n",
    "            # Cap so we don't go negative\n",
    "            if position - signal_strength < 0:\n",
    "                signal_strength = position\n",
    "\n",
    "            if signal_strength > 0:\n",
    "                signal_type = 'Sell'\n",
    "                position -= signal_strength\n",
    "                last_trade_signal_type = signal_type\n",
    "                last_signal_date = idx.strftime('%Y-%m-%d')\n",
    "                last_signal_strength = round(signal_strength, 2)\n",
    "                print(f\"[{ticker}] Sell on {last_signal_date} (Strength: {last_signal_strength})\")\n",
    "\n",
    "        analysis_df.at[idx, 'Position'] = position\n",
    "\n",
    "    # Modify Position_Shifted to apply signals on the same day so that I can run after US close on T and set passive order for T+1 \n",
    "\n",
    "    # Remove the shift to apply the position on the same day\n",
    "    analysis_df['Position_Shifted'] = analysis_df['Position']  # Changed from shift(1)\n",
    "\n",
    "    # Compute returns\n",
    "    analysis_df['Log_Return'] = analysis_df['Adj_Close'].diff()\n",
    "    analysis_df['Strategy_Return'] = analysis_df['Position_Shifted'] * analysis_df['Log_Return']\n",
    "    analysis_df['Strategy_Return'].fillna(0.0, inplace=True)\n",
    "\n",
    "    # Cumulative returns\n",
    "    analysis_df['Cumulative_Strategy_Return'] = analysis_df['Strategy_Return'].cumsum().apply(np.exp)\n",
    "    analysis_df['Cumulative_Buy_and_Hold_Return'] = analysis_df['Log_Return'].cumsum().apply(np.exp)\n",
    "\n",
    "    # Performance stats\n",
    "    strategy_total_return = analysis_df['Cumulative_Strategy_Return'].iloc[-1] - 1\n",
    "    buy_hold_total_return = analysis_df['Cumulative_Buy_and_Hold_Return'].iloc[-1] - 1\n",
    "    strategy_years = (analysis_df.index[-1] - analysis_df.index[0]).days / 365.25\n",
    "\n",
    "    if strategy_years <= 0:\n",
    "        strategy_annual_return = 0.0\n",
    "        buy_hold_annual_return = 0.0\n",
    "    else:\n",
    "        strategy_annual_return = (analysis_df['Cumulative_Strategy_Return'].iloc[-1])**(1/strategy_years) - 1\n",
    "        buy_hold_annual_return = (analysis_df['Cumulative_Buy_and_Hold_Return'].iloc[-1])**(1/strategy_years) - 1\n",
    "\n",
    "    strategy_volatility = analysis_df['Strategy_Return'].std() * np.sqrt(252)\n",
    "    buy_hold_volatility = analysis_df['Log_Return'].std() * np.sqrt(252)\n",
    "\n",
    "    strategy_sharpe = strategy_annual_return / strategy_volatility if strategy_volatility != 0 else 0.0\n",
    "    buy_hold_sharpe = buy_hold_annual_return / buy_hold_volatility if buy_hold_volatility != 0 else 0.0\n",
    "\n",
    "    # Drawdown\n",
    "    analysis_df['Strategy_Cumulative'] = analysis_df['Strategy_Return'].cumsum().apply(np.exp)\n",
    "    analysis_df['Strategy_Cumulative_Max'] = analysis_df['Strategy_Cumulative'].cummax()\n",
    "    analysis_df['Strategy_Drawdown'] = analysis_df['Strategy_Cumulative'] / analysis_df['Strategy_Cumulative_Max'] - 1\n",
    "    strategy_max_drawdown = analysis_df['Strategy_Drawdown'].min()\n",
    "\n",
    "    analysis_df['Buy_Hold_Cumulative'] = analysis_df['Log_Return'].cumsum().apply(np.exp)\n",
    "    analysis_df['Buy_Hold_Cumulative_Max'] = analysis_df['Buy_Hold_Cumulative'].cummax()\n",
    "    analysis_df['Buy_Hold_Drawdown'] = analysis_df['Buy_Hold_Cumulative'] / analysis_df['Buy_Hold_Cumulative_Max'] - 1\n",
    "    buy_hold_max_drawdown = analysis_df['Buy_Hold_Drawdown'].min()\n",
    "\n",
    "    performance_metrics = {\n",
    "        'Strategy Annual Return': strategy_annual_return,\n",
    "        'Buy and Hold Annual Return': buy_hold_annual_return,\n",
    "        'Strategy Sharpe': strategy_sharpe,\n",
    "        'Buy and Hold Sharpe': buy_hold_sharpe,\n",
    "        'Strategy Maximum Drawdown': strategy_max_drawdown,\n",
    "        'Buy and Hold Maximum Drawdown': buy_hold_max_drawdown\n",
    "    }\n",
    "    return performance_metrics, (last_trade_signal_type, last_signal_date, last_signal_strength)\n",
    "\n",
    "\n",
    "# Analyze Momentum\n",
    "\n",
    "def analyze_momentum(decompose_result, original_series, ticker='', description=''):\n",
    "    \"\"\"\n",
    "    Analyzes momentum based on the trend and residual components,\n",
    "    plus backtests the strategy.\n",
    "\n",
    "    Returns:\n",
    "      - trend_status\n",
    "      - residual_status\n",
    "      - performance_metrics\n",
    "      - last_trade_signal_details\n",
    "    \"\"\"\n",
    "    trend = decompose_result.trend\n",
    "    residual = decompose_result.resid\n",
    "\n",
    "    # Trend slope\n",
    "    slope_series = calculate_trend_slope_vectorized(trend, window=252)\n",
    "    # Residual z-score\n",
    "    z_score = calculate_residual_zscore(residual, window=252).rename('Residual_Z_Score')\n",
    "\n",
    "    # Combine\n",
    "    analysis_df = pd.concat([\n",
    "        original_series.rename('Adj_Close'),\n",
    "        trend.rename('Trend'),\n",
    "        slope_series.rename('Trend_Slope'),\n",
    "        residual.rename('Residual'),\n",
    "        z_score\n",
    "    ], axis=1)\n",
    "\n",
    "    # Plot Trend Slope\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(analysis_df.index, analysis_df['Trend_Slope'], color='purple', label='Trend Slope (1Yr Rolling)')\n",
    "    plt.title(f'Trend Slope - {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Slope')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, f\"{ticker}_trend_slope.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Residual Z-Score\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(analysis_df.index, analysis_df['Residual_Z_Score'], color='brown', label='Residual Z-Score (1Yr Rolling)')\n",
    "    plt.axhline(2, color='red', linestyle='--', label='±2')\n",
    "    plt.axhline(-2, color='green', linestyle='--')\n",
    "    plt.axhline(3, color='darkred', linestyle='--', label='±3')\n",
    "    plt.axhline(-3, color='darkgreen', linestyle='--')\n",
    "    plt.axhline(4, color='maroon', linestyle='--', label='±4')\n",
    "    plt.axhline(-4, color='darkolivegreen', linestyle='--')\n",
    "    plt.title(f'Residual Z-Score - {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Z-Score')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, f\"{ticker}_residual_zscore.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Interpretation\n",
    "    trend_status = \"Insufficient Data\"\n",
    "    residual_status = \"Insufficient Data\"\n",
    "    if not slope_series.dropna().empty and not z_score.dropna().empty:\n",
    "        latest_slope = slope_series.dropna().iloc[-1]\n",
    "        latest_z = z_score.dropna().iloc[-1]\n",
    "\n",
    "        # Trend Status\n",
    "        if latest_slope > 0:\n",
    "            trend_status = \"Upward Momentum\"\n",
    "        elif latest_slope < 0:\n",
    "            trend_status = \"Downward Momentum\"\n",
    "        else:\n",
    "            trend_status = \"No Clear Momentum\"\n",
    "\n",
    "        # Residual Status with multi-level thresholds\n",
    "        if latest_z > 4:\n",
    "            residual_status = \"Highly Overbought Condition\"\n",
    "        elif latest_z > 3:\n",
    "            residual_status = \"Moderately Overbought Condition\"\n",
    "        elif latest_z > 2:\n",
    "            residual_status = \"Slightly Overbought Condition\"\n",
    "        elif latest_z < -4:\n",
    "            residual_status = \"Highly Oversold Condition\"\n",
    "        elif latest_z < -3:\n",
    "            residual_status = \"Moderately Oversold Condition\"\n",
    "        elif latest_z < -2:\n",
    "            residual_status = \"Slightly Oversold Condition\"\n",
    "        else:\n",
    "            residual_status = \"Residuals Within Normal Range\"\n",
    "\n",
    "    # Backtest\n",
    "    performance_metrics, last_trade_signal_details = backtest_strategy(analysis_df, ticker, description)\n",
    "\n",
    "    return trend_status, residual_status, performance_metrics, last_trade_signal_details\n",
    "\n",
    "\n",
    "# Process a Single Ticker\n",
    "\n",
    "def process_ticker(ticker, description, adj_close_data):\n",
    "    \"\"\"\n",
    "    Runs the entire pipeline for a single ticker:\n",
    "      - Preprocess\n",
    "      - Log transform\n",
    "      - Decompose\n",
    "      - Analyze momentum\n",
    "      - Backtest\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'Ticker': ticker,\n",
    "        'Description': description,\n",
    "        'Trend Status': 'N/A',\n",
    "        'Residual Status': 'N/A',\n",
    "        'Strategy Annual Return': np.nan,\n",
    "        'Buy and Hold Annual Return': np.nan,\n",
    "        'Strategy Sharpe': np.nan,\n",
    "        'Buy and Hold Sharpe': np.nan,\n",
    "        'Strategy Maximum Drawdown': np.nan,\n",
    "        'Buy and Hold Maximum Drawdown': np.nan,\n",
    "        'Last Trade Signal': 'N/A',\n",
    "        'Last Signal Date': 'N/A',\n",
    "        'Last Signal Strength': 'N/A'\n",
    "    }\n",
    "\n",
    "    if ticker not in adj_close_data.columns:\n",
    "        return result\n",
    "\n",
    "    ticker_series = adj_close_data[ticker].dropna()\n",
    "    if ticker_series.empty:\n",
    "        return result\n",
    "\n",
    "    # 1) Preprocess\n",
    "    preprocessed_series = preprocess_ticker_data(ticker_series)\n",
    "    # 2) Log transform\n",
    "    log_series = log_transform(preprocessed_series)\n",
    "\n",
    "    # 3) Decompose\n",
    "    try:\n",
    "        additive_result = decompose_time_series(log_series, model='additive', period=252)\n",
    "    except Exception:\n",
    "        return result  # not enough data or decomposition error\n",
    "\n",
    "    # 4) Plot Decomposition\n",
    "    plot_decomposition(\n",
    "        additive_result,\n",
    "        title='Additive Decomposition of Log-Transformed Data',\n",
    "        ticker=ticker\n",
    "    )\n",
    "\n",
    "    # 5) Analyze Momentum & Backtest\n",
    "    trend_status, residual_status, performance_metrics, last_trade_signal_details = \\\n",
    "        analyze_momentum(additive_result, log_series, ticker, description)\n",
    "\n",
    "    # Update result\n",
    "    result.update({\n",
    "        'Trend Status': trend_status,\n",
    "        'Residual Status': residual_status,\n",
    "        'Strategy Annual Return': performance_metrics['Strategy Annual Return'],\n",
    "        'Buy and Hold Annual Return': performance_metrics['Buy and Hold Annual Return'],\n",
    "        'Strategy Sharpe': performance_metrics['Strategy Sharpe'],\n",
    "        'Buy and Hold Sharpe': performance_metrics['Buy and Hold Sharpe'],\n",
    "        'Strategy Maximum Drawdown': performance_metrics['Strategy Maximum Drawdown'],\n",
    "        'Buy and Hold Maximum Drawdown': performance_metrics['Buy and Hold Maximum Drawdown']\n",
    "    })\n",
    "\n",
    "    last_trade_signal_type, last_signal_date, last_signal_strength = last_trade_signal_details\n",
    "    if last_trade_signal_type in ['Buy', 'Sell']:\n",
    "        result['Last Trade Signal'] = last_trade_signal_type\n",
    "        result['Last Signal Date'] = last_signal_date\n",
    "        result['Last Signal Strength'] = last_signal_strength\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Main Execution\n",
    "# --------------------------------------------------------------------------\n",
    "def main():\n",
    "    if not TICKERS:\n",
    "        print(\"No tickers found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 1) Fetch data from yfinance\n",
    "    print(\"\\nFetching data from yfinance...\")\n",
    "    adj_close_data = fetch_data_with_yfinance(\n",
    "        tickers=TICKERS,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE\n",
    "    )\n",
    "\n",
    "    # Filter only those tickers that actually returned data\n",
    "    common_tickers = [t for t in TICKERS if t in adj_close_data.columns]\n",
    "    if not common_tickers:\n",
    "        print(\"No matching tickers found in yfinance data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    adj_close_data = adj_close_data[common_tickers]\n",
    "\n",
    "    # 2) Parallel processing\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    print(f\"Using {num_cores} CPU cores for parallel processing.\")\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_ticker)(\n",
    "            ticker,\n",
    "            TICKER_DESCRIPTIONS.get(ticker, 'No Description Available'),\n",
    "            adj_close_data\n",
    "        )\n",
    "        for ticker in common_tickers\n",
    "    )\n",
    "\n",
    "    # 3) Prepare and save results\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        columns_order = [\n",
    "            'Ticker',\n",
    "            'Description',\n",
    "            'Trend Status',\n",
    "            'Residual Status',\n",
    "            'Strategy Annual Return',\n",
    "            'Buy and Hold Annual Return',\n",
    "            'Strategy Sharpe',\n",
    "            'Buy and Hold Sharpe',\n",
    "            'Strategy Maximum Drawdown',\n",
    "            'Buy and Hold Maximum Drawdown',\n",
    "            'Last Trade Signal',\n",
    "            'Last Signal Date',\n",
    "            'Last Signal Strength'\n",
    "        ]\n",
    "        results_df = results_df[columns_order]\n",
    "        results_df.to_excel(OUTPUT_FILE_PATH, index=False)\n",
    "        print(f\"\\nAnalysis results saved to '{OUTPUT_FILE_PATH}'.\")\n",
    "    else:\n",
    "        print(\"No results generated. Exiting.\")\n",
    "\n",
    "    print(\"\\nAll tickers processed successfully.\")\n",
    "\n",
    "\n",
    "# Run Main\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2c5b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your_api_key_here\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"API_KEY\"] = \"your_api_key_here\"  # Set your API key here\n",
    "\n",
    "# Example: Accessing it right after setting\n",
    "print(os.environ[\"API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6662709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done – file saved & Sheet updated.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  QI – DAILY MERGER + FMP ENRICHMENT (Alpha × Reversion × Fundamentals)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Runs on local/server with VSCode.  Steps\n",
    "1. Load Alpha‐strategy Excel (same‑day)\n",
    "2. Load Reversion CSV (same‑day)\n",
    "3. Merge, add ISIN\n",
    "4. Fetch FMP profile + latest price — add marketCap, sector, industry, beta, price, volAvg, etc.\n",
    "5. Create dated folder under <QI_ROOT>/<YYYY‑MM‑DD>/\n",
    "6. Save enriched CSV + push to Google Sheet tab  “QI_S&P”\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd, numpy as np, requests, time, logging, os, re, sys\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from gspread.exceptions import APIError\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CONFIG - Make sure these paths are correct on your local system\n",
    "# ------------------------------------------------------------------\n",
    "FMP_API_KEY   = os.environ.get(\"API_KEY\")\n",
    "FMP_BATCH     = 100\n",
    "MAX_RETRIES   = 2\n",
    "LOG_PATH      = \"/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/script.log\"  # Local log file\n",
    "QI_ROOT       = \"/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output\"   # Make sure this exists or gets created\n",
    "SA_PATH       = \"/workspaces/Stock-Market-Prediction/Data/Google Sheets - #webook omitted"  # Local path to your Google Service Account JSON\n",
    "GSHEET_TAB    = \"QI_S&P\"\n",
    "BASE_DIR      = QI_ROOT         # Same as above unless different\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# LOGGING\n",
    "# ------------------------------------------------------------------\n",
    "logging.basicConfig(filename=LOG_PATH,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def extract_date_from_filename(fname):\n",
    "    m = re.search(r\"(\\d{4}-\\d{2}-\\d{2})\", fname)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def auth_gsheets(json_path):\n",
    "    scopes = ['https://www.googleapis.com/auth/spreadsheets',\n",
    "              'https://www.googleapis.com/auth/drive']\n",
    "    creds  = Credentials.from_service_account_file(json_path, scopes=scopes)\n",
    "    return gspread.authorize(creds)\n",
    "\n",
    "def gs_update(client, sheet_name, df, retries=5):\n",
    "    sh = client.open(sheet_name).sheet1\n",
    "    sh.clear()\n",
    "    payload = [list(df.columns)] + df.values.tolist()\n",
    "    backoff = 1\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            sh.update('A1', payload)\n",
    "            return\n",
    "        except APIError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "            else:\n",
    "                raise\n",
    "    logging.error(\"Google Sheet update failed after retries\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  FMP helpers\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def fetch_profiles(batch):\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/profile/{','.join(batch)}?apikey={FMP_API_KEY}\"\n",
    "    for n in range(MAX_RETRIES+1):\n",
    "        r = requests.get(url, timeout=15)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(0.5*(n+1))\n",
    "    return []\n",
    "\n",
    "def fetch_last_price(batch):\n",
    "    url = (\"https://financialmodelingprep.com/api/v3/historical-price-full/\"\n",
    "           f\"{','.join(batch)}?serietype=line&timeseries=1&apikey={FMP_API_KEY}\")\n",
    "    for n in range(MAX_RETRIES+1):\n",
    "        r = requests.get(url, timeout=15)\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get('historicalStockList', [])\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(0.5*(n+1))\n",
    "    return []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MAIN\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # Today’s date & paths ------------------------------\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "    alpha_excel = f\"{BASE_DIR}/analysis_results_{today_str}.xlsx\"\n",
    "    rev_csv     = f\"{QI_ROOT}/Reversion/{today_str}/sp500_mr_momentum_results_with_signal.csv\"\n",
    "    isin_csv    = f\"/workspaces/Stock-Market-Prediction/Data/S&P500 Stocks with ISIN.csv\"\n",
    "\n",
    "    # Folder for today\n",
    "    daily_dir   = os.path.join(QI_ROOT, today_str)\n",
    "    os.makedirs(daily_dir, exist_ok=True)\n",
    "\n",
    "    out_csv     = os.path.join(daily_dir, f\"QI_S&P_{today_str}.csv\")\n",
    "\n",
    "    # Load Alpha files ----------------------------------\n",
    "    if not os.path.exists(alpha_excel):\n",
    "        raise FileNotFoundError(alpha_excel)\n",
    "    if not os.path.exists(rev_csv):\n",
    "        raise FileNotFoundError(rev_csv)\n",
    "\n",
    "    alpha_df = pd.read_excel(alpha_excel, engine='openpyxl')\n",
    "    rev_df   = pd.read_csv(rev_csv)\n",
    "\n",
    "    alpha_df['Ticker'] = alpha_df['Ticker'].str.upper()\n",
    "    if 'ticker' in rev_df.columns:\n",
    "        rev_df.rename(columns={'ticker':'Ticker'}, inplace=True)\n",
    "    rev_df['Ticker']  = rev_df['Ticker'].str.upper()\n",
    "\n",
    "    alpha_df.set_index('Ticker', inplace=True, drop=False)\n",
    "    rev_df.set_index('Ticker',   inplace=True, drop=False)\n",
    "    new_cols = [c for c in rev_df.columns if c not in alpha_df.columns]\n",
    "    merged   = alpha_df.join(rev_df[new_cols], how='left').reset_index(drop=True)\n",
    "\n",
    "    # Add ISIN ------------------------------------------\n",
    "    isin_df = pd.read_csv(isin_csv)\n",
    "    isin_df['Symbol'] = isin_df['Symbol'].str.upper()\n",
    "    merged = merged.merge(isin_df[['Symbol','International Securities Identification Number']],\n",
    "                          left_on='Ticker', right_on='Symbol', how='left')\n",
    "    merged.drop(columns='Symbol', inplace=True)\n",
    "    merged.rename(columns={'International Securities Identification Number':'ISIN'}, inplace=True)\n",
    "\n",
    "    # Get Data from FMP such as industry, sector, market cap, beta, volume average for execution and portfolio optimisation ------------------------------------\n",
    "    prof_rows, price_rows = [], []\n",
    "    tickers = merged['Ticker'].unique().tolist()\n",
    "    for batch in _chunks(tickers, FMP_BATCH):\n",
    "        prof_rows.extend(fetch_profiles(batch))\n",
    "        price_rows.extend(fetch_last_price(batch))\n",
    "\n",
    "    prof_map = {'symbol':'Ticker','mktCap':'marketCap','sector':'sector','industry':'industry',\n",
    "                'beta':'beta','price':'price','volAvg':'volAvg'}\n",
    "    df_prof = pd.DataFrame(prof_rows)[list(prof_map.keys())].rename(columns=prof_map)\n",
    "    df_prof['Ticker'] = df_prof['Ticker'].str.upper()\n",
    "\n",
    "    df_price = (pd.json_normalize(price_rows, record_path='historical', meta=['symbol'])\n",
    "                .rename(columns={'symbol':'Ticker','close':'lastClose','vwap':'lastVWAP',\n",
    "                                 'changePercent':'lastChangePct','date':'lastPriceDate'}))\n",
    "    df_price['Ticker'] = df_price['Ticker'].str.upper()\n",
    "\n",
    "    merged = merged.merge(df_prof, on='Ticker', how='left').merge(df_price, on='Ticker', how='left')\n",
    "    merged['adv30Usd'] = merged['price']*merged['volAvg']\n",
    "\n",
    "    # Tidy & save ---------------------------------------\n",
    "    if 'Last Signal Date' in merged.columns:\n",
    "        merged['Last Signal Date'] = pd.to_datetime(merged['Last Signal Date'], errors='coerce')\n",
    "        merged.sort_values('Last Signal Date', ascending=False, inplace=True)\n",
    "    for c in merged.select_dtypes(include=['datetime64[ns]']).columns:\n",
    "        merged[c] = merged[c].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    merged.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
    "    merged.fillna(\"\", inplace=True)\n",
    "    merged.to_csv(out_csv, index=False)\n",
    "    logging.info(\"Saved enriched CSV → %s\", out_csv)\n",
    "\n",
    "    # Push to Google Sheet ------------------------------\n",
    "    client = auth_gsheets(SA_PATH)\n",
    "    gs_update(client, GSHEET_TAB, merged)\n",
    "    print(\"All done – file saved & Sheet updated.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
