{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa89cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Alpha Strategy Code Using yfinance + Enhanced Signal Logic\n",
    "# =============================================================================\n",
    "\n",
    "# Install Required Libraries (uncomment if needed)\n",
    "# !pip install pandas matplotlib statsmodels scikit-learn joblib numba openpyxl yfinance\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend for faster plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import yfinance as yf  # <-- yfinance for data\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# CONFIGURATION: CSV file for Tickers + Directory/Paths\n",
    "# --------------------------------------------------------------------------\n",
    "# CSV with \"Symbol\" and \"Description\" columns\n",
    "CSV_FILE_PATH = '/workspaces/Stock-Market-Prediction/Data/STOXX600 Tickers.csv'\n",
    "\n",
    "# Output paths\n",
    "today_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "OUTPUT_FILE_PATH = f'/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/analysis_results_STOXX_{today_date}.xlsx'\n",
    "PLOT_DIR = '/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Time range for yfinance data\n",
    "# Set END_DATE to tomorrow to include today's data, since yfinance's end date is exclusive\n",
    "END_DATE = (datetime.datetime.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "START_DATE = '1980-01-01'  # Adjust as needed\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Function to load Tickers from CSV\n",
    "# --------------------------------------------------------------------------\n",
    "def load_tickers(csv_path):\n",
    "    \"\"\"\n",
    "    Loads tickers and descriptions from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: List of ticker symbols.\n",
    "    - Dict[str, str]: Dictionary mapping ticker symbols to their descriptions.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"The CSV file '{csv_path}' does not exist. Please check the path.\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    required_columns = {'Symbol', 'Description'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV file must contain columns: {required_columns}\")\n",
    "\n",
    "    # Handle duplicates\n",
    "    df = df.drop_duplicates(subset='Symbol')\n",
    "\n",
    "    tickers = df['Symbol'].tolist()\n",
    "    descriptions = dict(zip(df['Symbol'], df['Description']))\n",
    "\n",
    "    print(f\"Loaded {len(tickers)} tickers from '{csv_path}'.\")\n",
    "    return tickers, descriptions\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Fetch Data via yfinance\n",
    "# --------------------------------------------------------------------------\n",
    "def fetch_data_with_yfinance(tickers, start_date='1980-01-01', end_date=None):\n",
    "    \"\"\"\n",
    "    Fetches historical Adjusted Close (or 'Close' if auto_adjust=True) prices\n",
    "    for all tickers using yfinance.\n",
    "\n",
    "    Parameters:\n",
    "    - tickers (List[str]): List of ticker symbols.\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format (default: tomorrow's date).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame (indexed by date) with columns = ticker symbols.\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = (datetime.datetime.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"Fetching data from yfinance for {len(tickers)} ticker(s)...\")\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        progress=False,\n",
    "        group_by='ticker',\n",
    "        auto_adjust=True\n",
    "    )\n",
    "\n",
    "    # If only one ticker, data won't be multi-index\n",
    "    if len(tickers) == 1:\n",
    "        adj_close_df = data['Close'].to_frame(name=tickers[0])\n",
    "    else:\n",
    "        # If multiple tickers, data is multi-index\n",
    "        adj_close_df = pd.DataFrame()\n",
    "        for tkr in tickers:\n",
    "            if ('Close' in data[tkr]):\n",
    "                adj_close_df[tkr] = data[tkr]['Close']\n",
    "\n",
    "    # Drop rows where all tickers are NaN\n",
    "    adj_close_df.dropna(how='all', inplace=True)\n",
    "\n",
    "    return adj_close_df\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Load Tickers and Descriptions\n",
    "# --------------------------------------------------------------------------\n",
    "try:\n",
    "    TICKERS, TICKER_DESCRIPTIONS = load_tickers(CSV_FILE_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tickers: {e}\")\n",
    "    TICKERS = []\n",
    "    TICKER_DESCRIPTIONS = {}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Preprocessing\n",
    "# --------------------------------------------------------------------------\n",
    "def preprocess_ticker_data(series):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for a single ticker by forward-filling,\n",
    "    ensuring a business-day frequency, and dropping NaN.\n",
    "    \"\"\"\n",
    "    series = series.dropna()\n",
    "    series = series.asfreq('B')\n",
    "    series.fillna(method='ffill', inplace=True)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(series.index):\n",
    "        series.index = pd.to_datetime(series.index)\n",
    "    return series\n",
    "\n",
    "\n",
    "def log_transform(series):\n",
    "    \"\"\"Apply natural log transform, avoiding log(0) errors.\"\"\"\n",
    "    series = series.replace(0, 1e-9)\n",
    "    return np.log(series)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Decomposition\n",
    "# --------------------------------------------------------------------------\n",
    "def decompose_time_series(series, model='additive', period=252):\n",
    "    \"\"\"\n",
    "    Decomposes the time series into trend, seasonal, and residual components.\n",
    "    \"\"\"\n",
    "    print(f\"Performing '{model}' decomposition (period={period})...\")\n",
    "    return seasonal_decompose(series, model=model, period=period, extrapolate_trend='freq')\n",
    "\n",
    "\n",
    "def plot_decomposition(result, title='Time Series Decomposition', ticker='', save_dir=PLOT_DIR):\n",
    "    \"\"\"Plot the decomposition results.\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.suptitle(f\"{title} - {ticker}\", fontsize=16)\n",
    "\n",
    "    plt.subplot(411)\n",
    "    plt.plot(result.observed, color='blue', label='Observed')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(412)\n",
    "    plt.plot(result.trend, color='orange', label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(413)\n",
    "    plt.plot(result.seasonal, color='green', label='Seasonal')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.subplot(414)\n",
    "    plt.plot(result.resid, color='red', label='Residual')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(os.path.join(save_dir, f\"{ticker}_decomposition.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Trend Slope Calculation (Vectorized Rolling Regression)\n",
    "# --------------------------------------------------------------------------\n",
    "def calculate_trend_slope_vectorized(trend_series, window=252):\n",
    "    \"\"\"\n",
    "    Calculates rolling linear regression slope on the trend component\n",
    "    using a vectorized approach.\n",
    "    \"\"\"\n",
    "    y = trend_series.dropna().values\n",
    "    n = len(y)\n",
    "    if n < window:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    try:\n",
    "        from numpy.lib.stride_tricks import sliding_window_view\n",
    "        rolling_windows = sliding_window_view(y, window_shape=window)\n",
    "    except AttributeError:\n",
    "        # fallback for older numpy versions\n",
    "        shape = (n - window + 1, window)\n",
    "        strides = (y.strides[0], y.strides[0])\n",
    "        rolling_windows = np.lib.stride_tricks.as_strided(y, shape=shape, strides=strides)\n",
    "\n",
    "    X = np.arange(window)\n",
    "    X_mean = X.mean()\n",
    "    X_var = ((X - X_mean) ** 2).sum()\n",
    "\n",
    "    Y_mean = rolling_windows.mean(axis=1)\n",
    "    cov = (rolling_windows - Y_mean[:, np.newaxis]) @ (X - X_mean)\n",
    "\n",
    "    slope = cov / X_var\n",
    "    idx = trend_series.dropna().index[window-1:]\n",
    "    return pd.Series(data=slope, index=idx)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Residual Z-Score\n",
    "# --------------------------------------------------------------------------\n",
    "def calculate_residual_zscore(residual_series, window=252):\n",
    "    \"\"\"\n",
    "    Calculates rolling Z-score of the residual component.\n",
    "    \"\"\"\n",
    "    rolling_mean = residual_series.rolling(window=window, min_periods=window).mean()\n",
    "    rolling_std = residual_series.rolling(window=window, min_periods=window).std()\n",
    "    z_score = (residual_series - rolling_mean) / rolling_std\n",
    "    return z_score\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Trend Strength Logic\n",
    "# --------------------------------------------------------------------------\n",
    "def get_trend_strength(slope):\n",
    "    \"\"\"\n",
    "    Determines a multiplier based on absolute slope.\n",
    "    \"\"\"\n",
    "    abs_slope = abs(slope)\n",
    "    if abs_slope < 0.01:\n",
    "        return 1.0  # Weak trend\n",
    "    elif 0.01 <= abs_slope < 0.05:\n",
    "        return 1.5  # Moderate trend\n",
    "    else:\n",
    "        return 2.0  # Strong trend\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Backtest Strategy with Variable Position Sizing\n",
    "# --------------------------------------------------------------------------\n",
    "def backtest_strategy(analysis_df, ticker='', description=''):\n",
    "    \"\"\"\n",
    "    Enhanced backtest:\n",
    "    - Buys incrementally when Z-score < -2\n",
    "    - Sells incrementally when Z-score > 2\n",
    "    - Position sizing is influenced by Z-score magnitude * trend strength.\n",
    "    - max_position = 3.0 (example) to limit total position.\n",
    "\n",
    "    Returns:\n",
    "    - performance_metrics: dict\n",
    "    - last_trade_signal_details: (signal_type, signal_date, signal_strength)\n",
    "    \"\"\"\n",
    "    analysis_df['Position'] = 0.0\n",
    "    last_trade_signal_type = 'N/A'\n",
    "    last_signal_date = 'N/A'\n",
    "    last_signal_strength = 'N/A'\n",
    "\n",
    "    position = 0.0\n",
    "    max_position = 3.0  # maximum position size\n",
    "    for idx, row in analysis_df.iterrows():\n",
    "        current_slope = row['Trend_Slope'] if not pd.isna(row['Trend_Slope']) else 0.0\n",
    "        trend_strength = get_trend_strength(current_slope)\n",
    "\n",
    "        z_score = row['Residual_Z_Score']\n",
    "        signal_strength = 0.0\n",
    "        signal_type = None\n",
    "\n",
    "        if z_score < -2:\n",
    "            # Potential Buy\n",
    "            signal_strength = abs(z_score) * trend_strength\n",
    "            # Cap at max position\n",
    "            if position + signal_strength > max_position:\n",
    "                signal_strength = max_position - position\n",
    "\n",
    "            if signal_strength > 0:\n",
    "                signal_type = 'Buy'\n",
    "                position += signal_strength\n",
    "                last_trade_signal_type = signal_type\n",
    "                last_signal_date = idx.strftime('%Y-%m-%d')\n",
    "                last_signal_strength = round(signal_strength, 2)\n",
    "                print(f\"[{ticker}] Buy on {last_signal_date} (Strength: {last_signal_strength})\")\n",
    "\n",
    "        elif z_score > 2:\n",
    "            # Potential Sell\n",
    "            signal_strength = z_score * trend_strength\n",
    "            # Cap so we don't go negative\n",
    "            if position - signal_strength < 0:\n",
    "                signal_strength = position\n",
    "\n",
    "            if signal_strength > 0:\n",
    "                signal_type = 'Sell'\n",
    "                position -= signal_strength\n",
    "                last_trade_signal_type = signal_type\n",
    "                last_signal_date = idx.strftime('%Y-%m-%d')\n",
    "                last_signal_strength = round(signal_strength, 2)\n",
    "                print(f\"[{ticker}] Sell on {last_signal_date} (Strength: {last_signal_strength})\")\n",
    "\n",
    "        analysis_df.at[idx, 'Position'] = position\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Modify Position_Shifted to apply signals on the same day\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Remove the shift to apply the position on the same day\n",
    "    analysis_df['Position_Shifted'] = analysis_df['Position']  # Changed from shift(1)\n",
    "\n",
    "    # Compute returns\n",
    "    analysis_df['Log_Return'] = analysis_df['Adj_Close'].diff()\n",
    "    analysis_df['Strategy_Return'] = analysis_df['Position_Shifted'] * analysis_df['Log_Return']\n",
    "    analysis_df['Strategy_Return'].fillna(0.0, inplace=True)\n",
    "\n",
    "    # Cumulative returns\n",
    "    analysis_df['Cumulative_Strategy_Return'] = analysis_df['Strategy_Return'].cumsum().apply(np.exp)\n",
    "    analysis_df['Cumulative_Buy_and_Hold_Return'] = analysis_df['Log_Return'].cumsum().apply(np.exp)\n",
    "\n",
    "    # Performance stats\n",
    "    strategy_total_return = analysis_df['Cumulative_Strategy_Return'].iloc[-1] - 1\n",
    "    buy_hold_total_return = analysis_df['Cumulative_Buy_and_Hold_Return'].iloc[-1] - 1\n",
    "    strategy_years = (analysis_df.index[-1] - analysis_df.index[0]).days / 365.25\n",
    "\n",
    "    if strategy_years <= 0:\n",
    "        strategy_annual_return = 0.0\n",
    "        buy_hold_annual_return = 0.0\n",
    "    else:\n",
    "        strategy_annual_return = (analysis_df['Cumulative_Strategy_Return'].iloc[-1])**(1/strategy_years) - 1\n",
    "        buy_hold_annual_return = (analysis_df['Cumulative_Buy_and_Hold_Return'].iloc[-1])**(1/strategy_years) - 1\n",
    "\n",
    "    strategy_volatility = analysis_df['Strategy_Return'].std() * np.sqrt(252)\n",
    "    buy_hold_volatility = analysis_df['Log_Return'].std() * np.sqrt(252)\n",
    "\n",
    "    strategy_sharpe = strategy_annual_return / strategy_volatility if strategy_volatility != 0 else 0.0\n",
    "    buy_hold_sharpe = buy_hold_annual_return / buy_hold_volatility if buy_hold_volatility != 0 else 0.0\n",
    "\n",
    "    # Drawdown\n",
    "    analysis_df['Strategy_Cumulative'] = analysis_df['Strategy_Return'].cumsum().apply(np.exp)\n",
    "    analysis_df['Strategy_Cumulative_Max'] = analysis_df['Strategy_Cumulative'].cummax()\n",
    "    analysis_df['Strategy_Drawdown'] = analysis_df['Strategy_Cumulative'] / analysis_df['Strategy_Cumulative_Max'] - 1\n",
    "    strategy_max_drawdown = analysis_df['Strategy_Drawdown'].min()\n",
    "\n",
    "    analysis_df['Buy_Hold_Cumulative'] = analysis_df['Log_Return'].cumsum().apply(np.exp)\n",
    "    analysis_df['Buy_Hold_Cumulative_Max'] = analysis_df['Buy_Hold_Cumulative'].cummax()\n",
    "    analysis_df['Buy_Hold_Drawdown'] = analysis_df['Buy_Hold_Cumulative'] / analysis_df['Buy_Hold_Cumulative_Max'] - 1\n",
    "    buy_hold_max_drawdown = analysis_df['Buy_Hold_Drawdown'].min()\n",
    "\n",
    "    performance_metrics = {\n",
    "        'Strategy Annual Return': strategy_annual_return,\n",
    "        'Buy and Hold Annual Return': buy_hold_annual_return,\n",
    "        'Strategy Sharpe': strategy_sharpe,\n",
    "        'Buy and Hold Sharpe': buy_hold_sharpe,\n",
    "        'Strategy Maximum Drawdown': strategy_max_drawdown,\n",
    "        'Buy and Hold Maximum Drawdown': buy_hold_max_drawdown\n",
    "    }\n",
    "    return performance_metrics, (last_trade_signal_type, last_signal_date, last_signal_strength)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Analyze Momentum\n",
    "# --------------------------------------------------------------------------\n",
    "def analyze_momentum(decompose_result, original_series, ticker='', description=''):\n",
    "    \"\"\"\n",
    "    Analyzes momentum based on the trend and residual components,\n",
    "    plus backtests the strategy.\n",
    "\n",
    "    Returns:\n",
    "      - trend_status\n",
    "      - residual_status\n",
    "      - performance_metrics\n",
    "      - last_trade_signal_details\n",
    "    \"\"\"\n",
    "    trend = decompose_result.trend\n",
    "    residual = decompose_result.resid\n",
    "\n",
    "    # Trend slope\n",
    "    slope_series = calculate_trend_slope_vectorized(trend, window=252)\n",
    "    # Residual z-score\n",
    "    z_score = calculate_residual_zscore(residual, window=252).rename('Residual_Z_Score')\n",
    "\n",
    "    # Combine\n",
    "    analysis_df = pd.concat([\n",
    "        original_series.rename('Adj_Close'),\n",
    "        trend.rename('Trend'),\n",
    "        slope_series.rename('Trend_Slope'),\n",
    "        residual.rename('Residual'),\n",
    "        z_score\n",
    "    ], axis=1)\n",
    "\n",
    "    # Plot Trend Slope\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(analysis_df.index, analysis_df['Trend_Slope'], color='purple', label='Trend Slope (1Yr Rolling)')\n",
    "    plt.title(f'Trend Slope - {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Slope')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, f\"{ticker}_trend_slope.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Residual Z-Score\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(analysis_df.index, analysis_df['Residual_Z_Score'], color='brown', label='Residual Z-Score (1Yr Rolling)')\n",
    "    plt.axhline(2, color='red', linestyle='--', label='±2')\n",
    "    plt.axhline(-2, color='green', linestyle='--')\n",
    "    plt.axhline(3, color='darkred', linestyle='--', label='±3')\n",
    "    plt.axhline(-3, color='darkgreen', linestyle='--')\n",
    "    plt.axhline(4, color='maroon', linestyle='--', label='±4')\n",
    "    plt.axhline(-4, color='darkolivegreen', linestyle='--')\n",
    "    plt.title(f'Residual Z-Score - {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Z-Score')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, f\"{ticker}_residual_zscore.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Interpretation\n",
    "    trend_status = \"Insufficient Data\"\n",
    "    residual_status = \"Insufficient Data\"\n",
    "    if not slope_series.dropna().empty and not z_score.dropna().empty:\n",
    "        latest_slope = slope_series.dropna().iloc[-1]\n",
    "        latest_z = z_score.dropna().iloc[-1]\n",
    "\n",
    "        # Trend Status\n",
    "        if latest_slope > 0:\n",
    "            trend_status = \"Upward Momentum\"\n",
    "        elif latest_slope < 0:\n",
    "            trend_status = \"Downward Momentum\"\n",
    "        else:\n",
    "            trend_status = \"No Clear Momentum\"\n",
    "\n",
    "        # Residual Status with multi-level thresholds\n",
    "        if latest_z > 4:\n",
    "            residual_status = \"Highly Overbought Condition\"\n",
    "        elif latest_z > 3:\n",
    "            residual_status = \"Moderately Overbought Condition\"\n",
    "        elif latest_z > 2:\n",
    "            residual_status = \"Slightly Overbought Condition\"\n",
    "        elif latest_z < -4:\n",
    "            residual_status = \"Highly Oversold Condition\"\n",
    "        elif latest_z < -3:\n",
    "            residual_status = \"Moderately Oversold Condition\"\n",
    "        elif latest_z < -2:\n",
    "            residual_status = \"Slightly Oversold Condition\"\n",
    "        else:\n",
    "            residual_status = \"Residuals Within Normal Range\"\n",
    "\n",
    "    # Backtest\n",
    "    performance_metrics, last_trade_signal_details = backtest_strategy(analysis_df, ticker, description)\n",
    "\n",
    "    return trend_status, residual_status, performance_metrics, last_trade_signal_details\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Process a Single Ticker\n",
    "# --------------------------------------------------------------------------\n",
    "def process_ticker(ticker, description, adj_close_data):\n",
    "    \"\"\"\n",
    "    Runs the entire pipeline for a single ticker:\n",
    "      - Preprocess\n",
    "      - Log transform\n",
    "      - Decompose\n",
    "      - Analyze momentum\n",
    "      - Backtest\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'Ticker': ticker,\n",
    "        'Description': description,\n",
    "        'Trend Status': 'N/A',\n",
    "        'Residual Status': 'N/A',\n",
    "        'Strategy Annual Return': np.nan,\n",
    "        'Buy and Hold Annual Return': np.nan,\n",
    "        'Strategy Sharpe': np.nan,\n",
    "        'Buy and Hold Sharpe': np.nan,\n",
    "        'Strategy Maximum Drawdown': np.nan,\n",
    "        'Buy and Hold Maximum Drawdown': np.nan,\n",
    "        'Last Trade Signal': 'N/A',\n",
    "        'Last Signal Date': 'N/A',\n",
    "        'Last Signal Strength': 'N/A'\n",
    "    }\n",
    "\n",
    "    if ticker not in adj_close_data.columns:\n",
    "        return result\n",
    "\n",
    "    ticker_series = adj_close_data[ticker].dropna()\n",
    "    if ticker_series.empty:\n",
    "        return result\n",
    "\n",
    "    # 1) Preprocess\n",
    "    preprocessed_series = preprocess_ticker_data(ticker_series)\n",
    "    # 2) Log transform\n",
    "    log_series = log_transform(preprocessed_series)\n",
    "\n",
    "    # 3) Decompose\n",
    "    try:\n",
    "        additive_result = decompose_time_series(log_series, model='additive', period=252)\n",
    "    except Exception:\n",
    "        return result  # not enough data or decomposition error\n",
    "\n",
    "    # 4) Plot Decomposition\n",
    "    plot_decomposition(\n",
    "        additive_result,\n",
    "        title='Additive Decomposition of Log-Transformed Data',\n",
    "        ticker=ticker\n",
    "    )\n",
    "\n",
    "    # 5) Analyze Momentum & Backtest\n",
    "    trend_status, residual_status, performance_metrics, last_trade_signal_details = \\\n",
    "        analyze_momentum(additive_result, log_series, ticker, description)\n",
    "\n",
    "    # Update result\n",
    "    result.update({\n",
    "        'Trend Status': trend_status,\n",
    "        'Residual Status': residual_status,\n",
    "        'Strategy Annual Return': performance_metrics['Strategy Annual Return'],\n",
    "        'Buy and Hold Annual Return': performance_metrics['Buy and Hold Annual Return'],\n",
    "        'Strategy Sharpe': performance_metrics['Strategy Sharpe'],\n",
    "        'Buy and Hold Sharpe': performance_metrics['Buy and Hold Sharpe'],\n",
    "        'Strategy Maximum Drawdown': performance_metrics['Strategy Maximum Drawdown'],\n",
    "        'Buy and Hold Maximum Drawdown': performance_metrics['Buy and Hold Maximum Drawdown']\n",
    "    })\n",
    "\n",
    "    last_trade_signal_type, last_signal_date, last_signal_strength = last_trade_signal_details\n",
    "    if last_trade_signal_type in ['Buy', 'Sell']:\n",
    "        result['Last Trade Signal'] = last_trade_signal_type\n",
    "        result['Last Signal Date'] = last_signal_date\n",
    "        result['Last Signal Strength'] = last_signal_strength\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Main Execution\n",
    "# --------------------------------------------------------------------------\n",
    "def main():\n",
    "    if not TICKERS:\n",
    "        print(\"No tickers found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 1) Fetch data from yfinance\n",
    "    print(\"\\nFetching data from yfinance...\")\n",
    "    adj_close_data = fetch_data_with_yfinance(\n",
    "        tickers=TICKERS,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE\n",
    "    )\n",
    "\n",
    "    # Filter only those tickers that actually returned data\n",
    "    common_tickers = [t for t in TICKERS if t in adj_close_data.columns]\n",
    "    if not common_tickers:\n",
    "        print(\"No matching tickers found in yfinance data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    adj_close_data = adj_close_data[common_tickers]\n",
    "\n",
    "    # 2) Parallel processing\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    print(f\"Using {num_cores} CPU cores for parallel processing.\")\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_ticker)(\n",
    "            ticker,\n",
    "            TICKER_DESCRIPTIONS.get(ticker, 'No Description Available'),\n",
    "            adj_close_data\n",
    "        )\n",
    "        for ticker in common_tickers\n",
    "    )\n",
    "\n",
    "    # 3) Prepare and save results\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        columns_order = [\n",
    "            'Ticker',\n",
    "            'Description',\n",
    "            'Trend Status',\n",
    "            'Residual Status',\n",
    "            'Strategy Annual Return',\n",
    "            'Buy and Hold Annual Return',\n",
    "            'Strategy Sharpe',\n",
    "            'Buy and Hold Sharpe',\n",
    "            'Strategy Maximum Drawdown',\n",
    "            'Buy and Hold Maximum Drawdown',\n",
    "            'Last Trade Signal',\n",
    "            'Last Signal Date',\n",
    "            'Last Signal Strength'\n",
    "        ]\n",
    "        results_df = results_df[columns_order]\n",
    "        results_df.to_excel(OUTPUT_FILE_PATH, index=False)\n",
    "        print(f\"\\nAnalysis results saved to '{OUTPUT_FILE_PATH}'.\")\n",
    "    else:\n",
    "        print(\"No results generated. Exiting.\")\n",
    "\n",
    "    print(\"\\nAll tickers processed successfully.\")\n",
    "\n",
    "\n",
    "# Run Main\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb9ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted date from filename: 2025-05-18\n",
      "Created or verified folder: /workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/2025-05-18\n",
      "Successfully read the main Excel file: '/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/analysis_results_STOXX_2025-05-18.xlsx'\n",
      "Successfully read the ISIN CSV file: '/workspaces/Stock-Market-Prediction/Data/STOXX Tickers with ISIN.csv'\n",
      "Successfully merged DataFrames.\n",
      "Successfully sorted the DataFrame by 'Last Signal Date' in descending order.\n",
      "Converted datetime column 'Last Signal Date' to string format.\n",
      "Successfully saved the merged data to '/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/2025-05-18/QI_S&P_2025-05-18.csv'\n",
      "Successfully updated the worksheet 'QI_STOXX600' in Google Sheet 'QI_S&P'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "import time\n",
    "import logging\n",
    "from gspread.exceptions import APIError, WorksheetNotFound, SpreadsheetNotFound\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/script.log',  \n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts a date in YYYY-MM-DD format from the given filename.\n",
    "    If no date is found, returns None.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def authenticate_google_sheets(credentials_path):\n",
    "    \"\"\"\n",
    "    Authenticates and returns a gspread client.\n",
    "    \"\"\"\n",
    "    SCOPES = [\n",
    "        'https://www.googleapis.com/auth/spreadsheets',\n",
    "        'https://www.googleapis.com/auth/drive'\n",
    "    ]\n",
    "    credentials = Credentials.from_service_account_file(\n",
    "        credentials_path,\n",
    "        scopes=SCOPES\n",
    "    )\n",
    "    client = gspread.authorize(credentials)\n",
    "    return client\n",
    "\n",
    "def update_google_sheet(client, sheet_name, dataframe, worksheet_title, max_retries=5):\n",
    "    \"\"\"\n",
    "    Updates the specified worksheet in the given Google Sheet with the provided DataFrame using batch update.\n",
    "    Implements exponential backoff for handling rate limits.\n",
    "\n",
    "    This function always updates the worksheet defined by worksheet_title.\n",
    "    If the worksheet does not exist, it creates it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spreadsheet = client.open(sheet_name)\n",
    "    except SpreadsheetNotFound:\n",
    "        logging.error(f\"The Google Sheet '{sheet_name}' was not found.\")\n",
    "        print(f\"Error: The Google Sheet '{sheet_name}' was not found.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while opening the Google Sheet: {e}\")\n",
    "        print(f\"An error occurred while opening the Google Sheet: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Get the desired worksheet\n",
    "    try:\n",
    "        worksheet = spreadsheet.worksheet(worksheet_title)\n",
    "    except WorksheetNotFound:\n",
    "        try:\n",
    "            # Create a new worksheet with default dimensions (adjust rows and cols as needed)\n",
    "            worksheet = spreadsheet.add_worksheet(title=worksheet_title, rows=\"100\", cols=\"20\")\n",
    "            logging.info(f\"Worksheet '{worksheet_title}' created in Google Sheet '{sheet_name}'.\")\n",
    "            print(f\"Worksheet '{worksheet_title}' created in Google Sheet '{sheet_name}'.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to create worksheet '{worksheet_title}': {e}\")\n",
    "            print(f\"Failed to create worksheet '{worksheet_title}': {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    # Clear existing content in the target worksheet\n",
    "    try:\n",
    "        worksheet.clear()\n",
    "        logging.info(f\"Cleared existing content in the worksheet '{worksheet_title}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to clear the worksheet: {e}\")\n",
    "        print(f\"Failed to clear the worksheet: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Prepare data for insertion\n",
    "    headers = list(dataframe.columns)\n",
    "    data = dataframe.values.tolist()\n",
    "    all_data = [headers] + data\n",
    "\n",
    "    # Determine the range to update (starting at A1)\n",
    "    cell_range = 'A1'\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            worksheet.update(cell_range, all_data)\n",
    "            logging.info(f\"Successfully updated the worksheet '{worksheet_title}' in Google Sheet '{sheet_name}'.\")\n",
    "            print(f\"Successfully updated the worksheet '{worksheet_title}' in Google Sheet '{sheet_name}'.\")\n",
    "            break\n",
    "        except APIError as api_err:\n",
    "            if api_err.response.status_code == 429:\n",
    "                wait_time = 2 ** attempt\n",
    "                logging.warning(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                logging.error(f\"An API error occurred: {api_err}\")\n",
    "                print(f\"An API error occurred: {api_err}\")\n",
    "                sys.exit(1)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred while updating the worksheet: {e}\")\n",
    "            print(f\"An unexpected error occurred while updating the worksheet: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logging.error(\"Max retries exceeded. Failed to update the worksheet.\")\n",
    "        print(\"Max retries exceeded. Failed to update the worksheet.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    main_excel_path = '/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output/analysis_results_STOXX_2025-05-18.xlsx'  # Path to your main Excel file\n",
    "    isin_csv_path = '/workspaces/Stock-Market-Prediction/Data/STOXX Tickers with ISIN.csv'  # Path to your ISIN CSV file\n",
    "    qi_folder_path = '/workspaces/Stock-Market-Prediction/Quantative Investment/QI Output'  # Path to the QI folder in Google Drive\n",
    "    service_account_path = '/workspaces/Stock-Market-Prediction/Data/Google Sheets - webhook-442401-7a057b78bd8f.json'  # Path to your service account JSON file\n",
    "    google_sheet_name = 'QI_S&P'  # Replace with your Google Sheet name\n",
    "\n",
    "    # Extract date from main_excel_path\n",
    "    date_str = extract_date_from_filename(main_excel_path)\n",
    "    if date_str:\n",
    "        logging.info(f\"Extracted date from filename: {date_str}\")\n",
    "        print(f\"Extracted date from filename: {date_str}\")\n",
    "    else:\n",
    "        date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "        logging.info(f\"No date found in filename. Using current date: {date_str}\")\n",
    "        print(f\"No date found in filename. Using current date: {date_str}\")\n",
    "\n",
    "    # Create a new folder in the QI directory for the date\n",
    "    new_folder_path = os.path.join(qi_folder_path, date_str)\n",
    "    try:\n",
    "        os.makedirs(new_folder_path, exist_ok=True)\n",
    "        logging.info(f\"Created or verified folder: {new_folder_path}\")\n",
    "        print(f\"Created or verified folder: {new_folder_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create or verify folder '{new_folder_path}': {e}\")\n",
    "        print(f\"Failed to create or verify folder '{new_folder_path}': {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Construct the output CSV file path\n",
    "    output_csv_path = os.path.join(new_folder_path, f\"QI_S&P_{date_str}.csv\")\n",
    "\n",
    "    # Read the main Excel file\n",
    "    try:\n",
    "        main_df = pd.read_excel(main_excel_path, engine='openpyxl')\n",
    "        logging.info(f\"Successfully read the main Excel file: '{main_excel_path}'\")\n",
    "        print(f\"Successfully read the main Excel file: '{main_excel_path}'\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"The file '{main_excel_path}' was not found.\")\n",
    "        print(f\"Error: The file '{main_excel_path}' was not found.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading '{main_excel_path}': {e}\")\n",
    "        print(f\"Error reading '{main_excel_path}': {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Read the ISIN CSV file\n",
    "    try:\n",
    "        isin_df = pd.read_csv(isin_csv_path)\n",
    "        logging.info(f\"Successfully read the ISIN CSV file: '{isin_csv_path}'\")\n",
    "        print(f\"Successfully read the ISIN CSV file: '{isin_csv_path}'\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"The file '{isin_csv_path}' was not found.\")\n",
    "        print(f\"Error: The file '{isin_csv_path}' was not found.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading '{isin_csv_path}': {e}\")\n",
    "        print(f\"Error reading '{isin_csv_path}': {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Ensure consistent case for merging\n",
    "    main_df['Ticker'] = main_df['Ticker'].str.upper()\n",
    "    isin_df['Symbol'] = isin_df['Symbol'].str.upper()\n",
    "\n",
    "    # Remove duplicate symbols in ISIN file if any\n",
    "    if isin_df['Symbol'].duplicated().any():\n",
    "        logging.warning(\"Duplicate symbols found in ISIN file. Keeping the first occurrence.\")\n",
    "        print(\"Warning: Duplicate symbols found in ISIN file. Keeping the first occurrence.\")\n",
    "        isin_df.drop_duplicates(subset=['Symbol'], keep='first', inplace=True)\n",
    "\n",
    "    # Merge the DataFrames on Ticker (from main_df) and Symbol (from isin_df)\n",
    "    try:\n",
    "        merged_df = pd.merge(\n",
    "            main_df,\n",
    "            isin_df[['Symbol', 'International Securities Identification Number']],\n",
    "            left_on='Ticker',\n",
    "            right_on='Symbol',\n",
    "            how='left'\n",
    "        )\n",
    "        logging.info(\"Successfully merged DataFrames.\")\n",
    "        print(\"Successfully merged DataFrames.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during merging DataFrames: {e}\")\n",
    "        print(f\"Error during merging DataFrames: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Drop the redundant 'Symbol' column and rename the ISIN column for clarity\n",
    "    merged_df.drop('Symbol', axis=1, inplace=True)\n",
    "    merged_df.rename(columns={'International Securities Identification Number': 'ISIN'}, inplace=True)\n",
    "\n",
    "    # Check for any missing ISINs\n",
    "    missing_isin = merged_df['ISIN'].isnull().sum()\n",
    "    if missing_isin > 0:\n",
    "        logging.warning(f\"{missing_isin} ticker(s) did not have a matching ISIN.\")\n",
    "        print(f\"Warning: {missing_isin} ticker(s) did not have a matching ISIN.\")\n",
    "\n",
    "    # Sort the DataFrame by 'Last Signal Date' from newest to oldest\n",
    "    date_column = 'Last Signal Date'\n",
    "    if date_column in merged_df.columns:\n",
    "        try:\n",
    "            merged_df[date_column] = pd.to_datetime(merged_df[date_column], errors='coerce')\n",
    "            merged_df.sort_values(by=date_column, ascending=False, inplace=True)\n",
    "            logging.info(f\"Successfully sorted the DataFrame by '{date_column}' in descending order.\")\n",
    "            print(f\"Successfully sorted the DataFrame by '{date_column}' in descending order.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sorting the DataFrame by '{date_column}': {e}\")\n",
    "            print(f\"Error sorting the DataFrame by '{date_column}': {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logging.warning(f\"The column '{date_column}' does not exist in the DataFrame.\")\n",
    "        print(f\"Warning: The column '{date_column}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Convert datetime columns to string for JSON serialization\n",
    "    for column in merged_df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(merged_df[column]):\n",
    "            try:\n",
    "                merged_df[column] = merged_df[column].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                logging.info(f\"Converted datetime column '{column}' to string format.\")\n",
    "                print(f\"Converted datetime column '{column}' to string format.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error converting column '{column}' to string: {e}\")\n",
    "                print(f\"Error converting column '{column}' to string: {e}\")\n",
    "                sys.exit(1)\n",
    "\n",
    "    # Replace infinite values with NaN and fill NaNs with an empty string\n",
    "    merged_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    merged_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    # Save the merged DataFrame to CSV\n",
    "    try:\n",
    "        merged_df.to_csv(output_csv_path, index=False)\n",
    "        logging.info(f\"Successfully saved the merged data to '{output_csv_path}'\")\n",
    "        print(f\"Successfully saved the merged data to '{output_csv_path}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving to '{output_csv_path}': {e}\")\n",
    "        print(f\"Error saving to '{output_csv_path}': {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Authenticate and update only the \"QI_STOXX600\" worksheet in the Google Sheet\n",
    "    try:\n",
    "        client = authenticate_google_sheets(service_account_path)\n",
    "        update_google_sheet(client, google_sheet_name, merged_df, worksheet_title=\"QI_STOXX600\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while updating the Google Sheet: {e}\")\n",
    "        print(f\"An error occurred while updating the Google Sheet: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
